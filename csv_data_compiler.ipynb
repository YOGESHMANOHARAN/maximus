{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory containing CSV files\n",
    "# csv_directory = \"D:\\Stuff\\Work\\maximus\\csv_files\"\n",
    "\n",
    "# # List to store the extracted data frames\n",
    "# dfs = []\n",
    "# current_time = 0  # Initialize the current time\n",
    "\n",
    "# # Iterate through all CSV files in the directory\n",
    "# for filename in sorted(os.listdir(csv_directory)):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         file_path = os.path.join(csv_directory, filename)\n",
    "        \n",
    "#         # Read the CSV file\n",
    "#         df = pd.read_csv(file_path)\n",
    "        \n",
    "#         # Check if \"down_short_hemisp\" column and \"time\" column exist in the file\n",
    "#         if \"down_short_hemisp\" in df.columns and \"time\" in df.columns:\n",
    "#             # Extract the desired columns\n",
    "#             df = df[[\"time\", \"down_short_hemisp\"]]\n",
    "            \n",
    "#             # Calculate the time offset for the current file\n",
    "#             time_offset = current_time\n",
    "            \n",
    "#             # Update the \"time\" column to create a continuous time index\n",
    "#             df[\"time\"] = df[\"time\"] + time_offset\n",
    "            \n",
    "#             # Append the extracted columns to the list\n",
    "#             dfs.append(df)\n",
    "            \n",
    "#             # Update the current_time for the next file\n",
    "#             current_time += 86400\n",
    "\n",
    "# # Concatenate all extracted data frames into a single data frame\n",
    "# result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # Sort the resulting data frame by the \"time\" column\n",
    "# result_df = result_df.sort_values(\"time\")\n",
    "\n",
    "# # Save the concatenated data frame to a new CSV file\n",
    "# result_df.to_csv(\"compiled_down_short_hemisp_with_continuous_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Directory containing CSV files\n",
    "# csv_directory = \"D:\\Stuff\\Work\\maximus\\csv_files\"\n",
    "\n",
    "# # List to store the extracted data frames\n",
    "# dfs = []\n",
    "# current_time = 0  # Initialize the current time\n",
    "\n",
    "# # Iterate through all CSV files in the directory\n",
    "# for filename in sorted(os.listdir(csv_directory)):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         file_path = os.path.join(csv_directory, filename)\n",
    "        \n",
    "#         # Extract the date from the file name\n",
    "#         date_str = filename.split(\".\")[2]\n",
    "#         file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "#         print(file_date[10:])\n",
    "#         break\n",
    "\n",
    "#         # Read the CSV file\n",
    "#         df = pd.read_csv(file_path)\n",
    "        \n",
    "#         # Check if \"down_short_hemisp\" column and \"time\" column exist in the file\n",
    "#         if \"down_short_hemisp\" in df.columns and \"time\" in df.columns:\n",
    "#             # Extract the desired columns\n",
    "#             df = df[[\"time\", \"down_short_hemisp\"]]\n",
    "            \n",
    "#             # Calculate the time offset for the current file\n",
    "#             time_offset = current_time\n",
    "            \n",
    "#             # Update the \"time\" column to create a continuous time index\n",
    "#             df[\"time\"] = df[\"time\"] + time_offset\n",
    "            \n",
    "#             current_date = file_date\n",
    "\n",
    "#             # Convert time index to a formatted date and time string\n",
    "#             df[\"time\"] = df[\"time\"].apply(lambda x: current_date + timedelta(seconds=x))\n",
    "#             #df[\"time\"] = df[\"time\"].apply(lambda x: file_date + timedelta(seconds=x))\n",
    "            \n",
    "#             # Append the extracted columns to the list\n",
    "#             dfs.append(df)\n",
    "            \n",
    "#             # Update the current_time for the next file\n",
    "#             current_time += 86400\n",
    "\n",
    "# # Concatenate all extracted data frames into a single data frame\n",
    "# result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # Sort the resulting data frame by the \"time\" column\n",
    "# result_df = result_df.sort_values(\"time\")\n",
    "\n",
    "# # Save the concatenated data frame to a new CSV file\n",
    "# result_df.to_csv(\"compiled_down_short_hemisp_with_continuous_time.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df[\"down_short_hemisp\"].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been compiled and saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Directory containing CSV files\n",
    "csv_directory = \"D:\\\\Stuff\\\\Work\\\\maximus\\\\csv_files\"\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract the base time (Unix timestamp) from the first row and convert it to a datetime object\n",
    "    base_time_unix = df.at[0, 'base_time']\n",
    "    base_datetime = datetime.utcfromtimestamp(base_time_unix)\n",
    "    \n",
    "    # Check if the year is above 2023, if so print the file_path\n",
    "    if base_datetime.year > 2023:\n",
    "        print(f\"Year exceeds 2023 in file: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if \"down_short_hemisp\" column and \"time_offset\" column exist in the file\n",
    "    if \"down_short_hemisp\" in df.columns and \"time_offset\" in df.columns:\n",
    "        df = df[[\"time_offset\", \"down_short_hemisp\"]]\n",
    "        df['time'] = df['time_offset'].apply(lambda x: base_datetime + timedelta(seconds=x))\n",
    "        df.drop('time_offset', axis=1, inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# List to store the processed data frames\n",
    "dfs = []\n",
    "\n",
    "# Iterate through all CSV files in the directory\n",
    "for filename in sorted(os.listdir(csv_directory)):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(csv_directory, filename)\n",
    "        \n",
    "        # Process each CSV file\n",
    "        df = process_csv(file_path)\n",
    "        \n",
    "        # Append the processed DataFrame to the list if it's not None\n",
    "        if df is not None:\n",
    "            dfs.append(df)\n",
    "\n",
    "# Concatenate all processed data frames into a single data frame\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort the resulting data frame by the \"time\" column\n",
    "result_df.sort_values(\"time\", inplace=True)\n",
    "\n",
    "result_df.index=result_df[\"time\"]\n",
    "\n",
    "result_df.drop(columns=[\"time\"], inplace=True)\n",
    "\n",
    "# Save the concatenated data frame to a new CSV file\n",
    "result_df.to_csv(\"31378194719419.csv\")\n",
    "\n",
    "print(f\"Data has been compiled and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bruh=result_df[[\"time\",\"down_short_hemisp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bruh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=bruh.copy()\n",
    "# df.drop(columns=[\"time\"],inplace=True)\n",
    "# df.index=bruh[\"time\"]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"Downwelling_Radiation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
